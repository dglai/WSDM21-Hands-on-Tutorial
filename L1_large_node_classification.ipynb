{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Training GNN with Neighbor Sampling for Node Classification\n",
    "===========================================================\n",
    "\n",
    "This tutorial shows how to train a multi-layer GraphSAGE for node\n",
    "classification on ``ogbn-arxiv`` provided by [Open Graph\n",
    "Benchmark (OGB)](https://ogb.stanford.edu/). The dataset contains around\n",
    "170 thousand nodes and 1 million edges.\n",
    "\n",
    "By the end of this tutorial, you will be able to\n",
    "\n",
    "-  Train a GNN model for node classification on a single GPU with DGL's\n",
    "   neighbor sampling components.\n",
    "\n",
    "This tutorial assumes that you have read the [Introduction of Neighbor\n",
    "Sampling for GNN Training](L0_neighbor_sampling_overview.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataset\n",
    "---------------\n",
    "\n",
    "OGB already prepared the data as DGL graph.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.08 GB: 100%|██████████| 81/81 [01:18<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/arxiv.zip\n",
      "Loading necessary files...\n",
      "This might take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14413.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 138.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graphs...\n",
      "Converting graphs into DGL objects...\n",
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "dataset = DglNodePropPredDataset('ogbn-arxiv')\n",
    "device = 'cpu'      # change to 'cuda' for GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OGB dataset is a collection of graphs and their labels. ``ogbn-arxiv``\n",
    "dataset only contains a single graph. So you can\n",
    "simply get the graph and its node labels like this:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=169343, num_edges=2332486,\n",
      "      ndata_schemes={'year': Scheme(shape=(1,), dtype=torch.int64), 'feat': Scheme(shape=(128,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "tensor([[ 4],\n",
      "        [ 5],\n",
      "        [28],\n",
      "        ...,\n",
      "        [10],\n",
      "        [ 4],\n",
      "        [ 1]])\n",
      "Number of classes: 40\n"
     ]
    }
   ],
   "source": [
    "graph, node_labels = dataset[0]\n",
    "# Add reverse edges since ogbn-arxiv is unidirectional.\n",
    "graph = dgl.add_reverse_edges(graph)\n",
    "graph.ndata['label'] = node_labels[:, 0]\n",
    "print(graph)\n",
    "print(node_labels)\n",
    "\n",
    "node_features = graph.ndata['feat']\n",
    "num_features = node_features.shape[1]\n",
    "num_classes = (node_labels.max() + 1).item()\n",
    "print('Number of classes:', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the training-validation-test split of the nodes with\n",
    "``get_split_idx`` method.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_split = dataset.get_idx_split()\n",
    "train_nids = idx_split['train']\n",
    "valid_nids = idx_split['valid']\n",
    "test_nids = idx_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How DGL Handles Computation Dependency\n",
    "--------------------------------------\n",
    "\n",
    "In the [previous tutorial](L0_neighbor_sampling_overview.ipynb), you\n",
    "have seen that the computation dependency for message passing of a\n",
    "single node can be described as a series of *message flow graphs* (MFG).\n",
    "\n",
    "![](https://data.dgl.ai/tutorial/img/bipartite.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Neighbor Sampler and Data Loader in DGL\n",
    "------------------------------------------------\n",
    "\n",
    "DGL provides tools to iterate over the dataset in minibatches\n",
    "while generating the computation dependencies to compute their outputs\n",
    "with the MFGs above. For node classification, you can use\n",
    "``dgl.dataloading.NodeDataLoader`` for iterating over the dataset.\n",
    "It accepts a sampler object to control how to generate the computation\n",
    "dependencies in the form of MFGs.  DGL provides\n",
    "implementations of common sampling algorithms such as\n",
    "``dgl.dataloading.MultiLayerNeighborSampler`` which randomly picks\n",
    "a fixed number of neighbors for each node.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Note**: To write your own neighbor sampler, please refer to [this user guide section](https://docs.dgl.ai/guide/minibatch-custom-sampler.html).\n",
    "    \n",
    "</div>\n",
    "\n",
    "The syntax of ``dgl.dataloading.NodeDataLoader`` is mostly similar to a\n",
    "PyTorch ``DataLoader``, with the addition that it needs a graph to\n",
    "generate computation dependency from, a set of node IDs to iterate on,\n",
    "and the neighbor sampler you defined.\n",
    "\n",
    "Let’s say that each node will gather messages from 4 neighbors on each\n",
    "layer. The code defining the data loader and neighbor sampler will look\n",
    "like the following.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4])\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    # The following arguments are specific to NodeDataLoader.\n",
    "    graph,              # The graph\n",
    "    train_nids,         # The node IDs to iterate over in minibatches\n",
    "    sampler,            # The neighbor sampler\n",
    "    device=device,      # Put the sampled MFGs on CPU or GPU\n",
    "    # The following arguments are inherited from PyTorch DataLoader.\n",
    "    batch_size=1024,    # Batch size\n",
    "    shuffle=True,       # Whether to shuffle the nodes for every epoch\n",
    "    drop_last=False,    # Whether to drop the last incomplete batch\n",
    "    num_workers=0       # Number of sampler processes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can iterate over the data loader and see what it yields.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 72643, 115210, 163116,  ...,  79731,  55737, 106865]), tensor([ 72643, 115210, 163116,  ...,  81095, 102068,  71131]), [Block(num_src_nodes=12467, num_dst_nodes=4042, num_edges=14540), Block(num_src_nodes=4042, num_dst_nodes=1024, num_edges=3225)]]\n",
      "To compute 1024 nodes' outputs, we need 12467 nodes' input features\n"
     ]
    }
   ],
   "source": [
    "input_nodes, output_nodes, mfgs = example_minibatch = next(iter(train_dataloader))\n",
    "print(example_minibatch)\n",
    "print(\"To compute {} nodes' outputs, we need {} nodes' input features\".format(len(output_nodes), len(input_nodes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``NodeDataLoader`` gives us three items per iteration.\n",
    "\n",
    "-  An ID tensor for the input nodes, i.e., nodes whose input features\n",
    "   are needed on the first GNN layer for this minibatch.\n",
    "-  An ID tensor for the output nodes, i.e. nodes whose representations\n",
    "   are to be computed.\n",
    "-  A list of MFGs storing the computation dependencies\n",
    "   for each GNN layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the source and destination node IDs of the MFGs\n",
    "and verify that the first few source nodes are always the same as the destination\n",
    "nodes.  As we described in the [overview](L0_neighbor_sampling_overview.ipynb),\n",
    "destination nodes' own features from the previous layer may also be necessary in\n",
    "the computation of the new features.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 72643, 115210, 163116,  ...,  79731,  55737, 106865])\n",
      "tensor([ 72643, 115210, 163116,  ...,  60885, 107551, 153275])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "mfg_0_src = mfgs[0].srcdata[dgl.NID]\n",
    "mfg_0_dst = mfgs[0].dstdata[dgl.NID]\n",
    "print(mfg_0_src)\n",
    "print(mfg_0_dst)\n",
    "print(torch.equal(mfg_0_src[:mfgs[0].num_dst_nodes()], mfg_0_dst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Model\n",
    "--------------\n",
    "\n",
    "Let’s consider training a 2-layer GraphSAGE with neighbor sampling. The\n",
    "model can be written as follows:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import SAGEConv\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, aggregator_type='mean')\n",
    "        self.conv2 = SAGEConv(h_feats, num_classes, aggregator_type='mean')\n",
    "        self.h_feats = h_feats\n",
    "\n",
    "    def forward(self, mfgs, x):\n",
    "        # Lines that are changed are marked with an arrow: \"<---\"\n",
    "\n",
    "        h_dst = x[:mfgs[0].num_dst_nodes()]  # <---\n",
    "        h = self.conv1(mfgs[0], (x, h_dst))  # <---\n",
    "        h = F.relu(h)\n",
    "        h_dst = h[:mfgs[1].num_dst_nodes()]  # <---\n",
    "        h = self.conv2(mfgs[1], (h, h_dst))  # <---\n",
    "        return h\n",
    "\n",
    "model = Model(num_features, 128, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare against the code in the\n",
    "[introduction](1_introduction.ipynb), you will notice several\n",
    "differences:\n",
    "\n",
    "-  **DGL GNN layers on MFGs**. Instead of computing on the\n",
    "   full graph:\n",
    "\n",
    "   ```python\n",
    "   h = self.conv1(g, x)\n",
    "   ```\n",
    "\n",
    "   you only compute on the sampled MFG:\n",
    "\n",
    "   ```python\n",
    "   h = self.conv1(mfgs[0], (x, h_dst))\n",
    "   ```\n",
    "   \n",
    "   All DGL’s GNN modules support message passing on MFGs,\n",
    "   where you supply a pair of features, one for source nodes and another\n",
    "   for destination nodes.\n",
    "\n",
    "-  **Feature slicing for self-dependency**. There are statements that\n",
    "   perform slicing to obtain the previous-layer representation of the\n",
    "    nodes:\n",
    "\n",
    "   ```python\n",
    "   h_dst = x[:mfgs[0].num_dst_nodes()]\n",
    "   ```\n",
    "\n",
    "   ``num_dst_nodes`` method works with MFGs, where it will\n",
    "   return the number of destination nodes.\n",
    "\n",
    "   Since the first few source nodes of the yielded MFG are\n",
    "   always the same as the destination nodes, these statements obtain the\n",
    "   representations of the destination nodes on the previous layer. They are\n",
    "   then combined with neighbor aggregation in ``dgl.nn.SAGEConv`` layer.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Note**: See the [custom message passing tutorial](L4_message_passing.ipynb) for more details on how to\n",
    "   manipulate MFGs produced in this way, such as the usage\n",
    "   of ``num_dst_nodes``.\n",
    "    \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Training Loop\n",
    "----------------------\n",
    "\n",
    "The following initializes the model and defines the optimizer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When computing the validation score for model selection, usually you can\n",
    "also do neighbor sampling. To do that, you need to define another data\n",
    "loader.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, valid_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a training loop that performs validation every epoch.\n",
    "It also saves the model with the best validation accuracy into a file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:01<00:00, 46.45it/s, loss=1.914, acc=0.496]\n",
      "100%|██████████| 30/30 [00:00<00:00, 67.69it/s]\n",
      "  6%|▌         | 5/89 [00:00<00:01, 46.20it/s, loss=1.772, acc=0.538]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.5395483069901674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:01<00:00, 53.54it/s, loss=1.535, acc=0.587]\n",
      "100%|██████████| 30/30 [00:00<00:00, 81.68it/s]\n",
      "  7%|▋         | 6/89 [00:00<00:01, 56.67it/s, loss=1.476, acc=0.573]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Accuracy 0.6097184469277492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:01<00:00, 59.88it/s, loss=1.334, acc=0.603]\n",
      "100%|██████████| 30/30 [00:00<00:00, 81.55it/s]\n",
      "  7%|▋         | 6/89 [00:00<00:01, 57.32it/s, loss=1.310, acc=0.616]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Accuracy 0.628443907513675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:01<00:00, 59.85it/s, loss=1.218, acc=0.645]\n",
      "100%|██████████| 30/30 [00:00<00:00, 81.80it/s]\n",
      "  7%|▋         | 6/89 [00:00<00:01, 57.25it/s, loss=1.274, acc=0.627]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Accuracy 0.639719453672942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:01<00:00, 57.93it/s, loss=1.226, acc=0.630]\n",
      "100%|██████████| 30/30 [00:00<00:00, 82.32it/s]\n",
      "  7%|▋         | 6/89 [00:00<00:01, 57.87it/s, loss=1.172, acc=0.651]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Accuracy 0.6398872445384073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:01<00:00, 59.76it/s, loss=1.201, acc=0.633]\n",
      "100%|██████████| 30/30 [00:00<00:00, 81.63it/s]\n",
      "  7%|▋         | 6/89 [00:00<00:01, 58.30it/s, loss=1.211, acc=0.637]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Accuracy 0.6510285580053022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:01<00:00, 59.85it/s, loss=1.181, acc=0.656]\n",
      "100%|██████████| 30/30 [00:00<00:00, 81.36it/s]\n",
      "  7%|▋         | 6/89 [00:00<00:01, 57.96it/s, loss=1.147, acc=0.646]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Accuracy 0.6522030940635591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:01<00:00, 59.67it/s, loss=1.173, acc=0.662]\n",
      "100%|██████████| 30/30 [00:00<00:00, 81.24it/s]\n",
      "  7%|▋         | 6/89 [00:00<00:01, 57.25it/s, loss=1.115, acc=0.664]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Accuracy 0.6567670056042149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:01<00:00, 59.74it/s, loss=1.211, acc=0.644]\n",
      "100%|██████████| 30/30 [00:00<00:00, 82.29it/s]\n",
      "  7%|▋         | 6/89 [00:00<00:01, 57.63it/s, loss=1.108, acc=0.678]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Accuracy 0.6638477801268499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:01<00:00, 52.51it/s, loss=1.154, acc=0.645]\n",
      "100%|██████████| 30/30 [00:00<00:00, 81.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Accuracy 0.6629417094533374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import sklearn.metrics\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = 'model.pt'\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "\n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, output_nodes, mfgs) in enumerate(tq):\n",
    "            # feature copy from CPU to GPU takes place here\n",
    "            inputs = mfgs[0].srcdata['feat']\n",
    "            labels = mfgs[-1].dstdata['label']\n",
    "\n",
    "            predictions = model(mfgs, inputs)\n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "\n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item(), 'acc': '%.03f' % accuracy}, refresh=False)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with tqdm.tqdm(valid_dataloader) as tq, torch.no_grad():\n",
    "        for input_nodes, output_nodes, mfgs in tq:\n",
    "            inputs = mfgs[0].srcdata['feat']\n",
    "            labels.append(mfgs[-1].dstdata['label'].cpu().numpy())\n",
    "            predictions.append(model(mfgs, inputs).argmax(1).cpu().numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "        if best_accuracy < accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "----------\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer GraphSAGE\n",
    "with neighbor sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
