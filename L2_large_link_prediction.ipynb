{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nStochastic Training of GNN for Link Prediction\n==============================================\n\nThis tutorial will show how to train a multi-layer GraphSAGE for link\nprediction on ``ogbn-arxiv`` provided by `Open Graph Benchmark\n(OGB) <https://ogb.stanford.edu/>`__. The dataset\ncontains around 170 thousand nodes and 1 million edges.\n\nBy the end of this tutorial, you will be able to\n\n-  Train a GNN model for link prediction on a single GPU with DGL's\n   neighbor sampling components.\n\nThis tutorial assumes that you have read the :doc:`Introduction of Neighbor\nSampling for GNN Training <L0_neighbor_sampling_overview>` and :doc:`Neighbor\nSampling for Node Classification <L1_large_node_classification>`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Link Prediction Overview\n------------------------\n\nLink prediction requires the model to predict the probability of\nexistence of an edge. This tutorial does so by computing a dot product\nbetween the representations of both incident nodes.\n\n\\begin{align}\\hat{y}_{u\\sim v} = \\sigma(h_u^T h_v)\\end{align}\n\nIt then minimizes the following binary cross entropy loss.\n\n\\begin{align}\\mathcal{L} = -\\sum_{u\\sim v\\in \\mathcal{D}}\\left( y_{u\\sim v}\\log(\\hat{y}_{u\\sim v}) + (1-y_{u\\sim v})\\log(1-\\hat{y}_{u\\sim v})) \\right)\\end{align}\n\nThis is identical to the link prediction formulation in :doc:`the previous\ntutorial on link prediction <../blitz/4_link_predict>`.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading Dataset\n---------------\n\nThis tutorial loads the dataset from the ``ogb`` package as in the\n:doc:`previous tutorial <L1_large_node_classification>`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import dgl\nimport torch\nimport numpy as np\nfrom ogb.nodeproppred import DglNodePropPredDataset\n\ndataset = DglNodePropPredDataset('ogbn-arxiv')\ndevice = 'cpu'      # change to 'cuda' for GPU\n\ngraph, node_labels = dataset[0]\n# Add reverse edges since ogbn-arxiv is unidirectional.\ngraph = dgl.add_reverse_edges(graph)\nprint(graph)\nprint(node_labels)\n\nnode_features = graph.ndata['feat']\nnode_labels = node_labels[:, 0]\nnum_features = node_features.shape[1]\nnum_classes = (node_labels.max() + 1).item()\nprint('Number of classes:', num_classes)\n\nidx_split = dataset.get_idx_split()\ntrain_nids = idx_split['train']\nvalid_nids = idx_split['valid']\ntest_nids = idx_split['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining Neighbor Sampler and Data Loader in DGL\n------------------------------------------------\n\nDifferent from the :doc:`link prediction tutorial for full\ngraph <../blitz/4_link_predict>`, a common practice to train GNN on large graphs is\nto iterate over the edges\nin minibatches, since computing the probability of all edges is usually\nimpossible. For each minibatch of edges, you compute the output\nrepresentation of their incident nodes using neighbor sampling and GNN,\nin a similar fashion introduced in the :doc:`large-scale node classification\ntutorial <L1_large_node_classification>`.\n\nDGL provides ``dgl.dataloading.EdgeDataLoader`` to\niterate over edges for edge classification or link prediction tasks.\n\nTo perform link prediction, you need to specify a negative sampler. DGL\nprovides builtin negative samplers such as\n``dgl.dataloading.negative_sampler.Uniform``.  Here this tutorial uniformly\ndraws 5 negative examples per positive example.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "negative_sampler = dgl.dataloading.negative_sampler.Uniform(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After defining the negative sampler, one can then define the edge data\nloader with neighbor sampling.  To create an ``EdgeDataLoader`` for\nlink prediction, provide a neighbor sampler object as well as the negative\nsampler object created above.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4])\ntrain_dataloader = dgl.dataloading.EdgeDataLoader(\n    # The following arguments are specific to NodeDataLoader.\n    graph,                                  # The graph\n    torch.arange(graph.number_of_edges()),  # The edges to iterate over\n    sampler,                                # The neighbor sampler\n    negative_sampler=negative_sampler,      # The negative sampler\n    device=device,                          # Put the MFGs on CPU or GPU\n    # The following arguments are inherited from PyTorch DataLoader.\n    batch_size=1024,    # Batch size\n    shuffle=True,       # Whether to shuffle the nodes for every epoch\n    drop_last=False,    # Whether to drop the last incomplete batch\n    num_workers=0       # Number of sampler processes\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can peek one minibatch from ``train_dataloader`` and see what it\nwill give you.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_nodes, pos_graph, neg_graph, mfgs = next(iter(train_dataloader))\nprint('Number of input nodes:', len(input_nodes))\nprint('Positive graph # nodes:', pos_graph.number_of_nodes(), '# edges:', pos_graph.number_of_edges())\nprint('Negative graph # nodes:', neg_graph.number_of_nodes(), '# edges:', neg_graph.number_of_edges())\nprint(mfgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The example minibatch consists of four elements.\n\nThe first element is an ID tensor for the input nodes, i.e., nodes\nwhose input features are needed on the first GNN layer for this minibatch.\n\nThe second element and the third element are the positive graph and the\nnegative graph for this minibatch.\nThe concept of positive and negative graphs have been introduced in the\n:doc:`full-graph link prediction tutorial <../blitz/4_link_predict>`.  In minibatch\ntraining, the positive graph and the negative graph only contain nodes\nnecessary for computing the pair-wise scores of positive and negative examples\nin the current minibatch.\n\nThe last element is a list of :doc:`MFGs <L0_neighbor_sampling_overview>`\nstoring the computation dependencies for each GNN layer.\nThe MFGs are used to compute the GNN outputs of the nodes\ninvolved in positive/negative graph.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining Model for Node Representation\n--------------------------------------\n\nThe model is almost identical to the one in the :doc:`node classification\ntutorial <L1_large_node_classification>`. The only difference is\nthat since you are doing link prediction, the output dimension will not\nbe the number of classes in the dataset.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\nimport torch.nn.functional as F\nfrom dgl.nn import SAGEConv\n\nclass Model(nn.Module):\n    def __init__(self, in_feats, h_feats):\n        super(Model, self).__init__()\n        self.conv1 = SAGEConv(in_feats, h_feats, aggregator_type='mean')\n        self.conv2 = SAGEConv(h_feats, h_feats, aggregator_type='mean')\n        self.h_feats = h_feats\n\n    def forward(self, mfgs, x):\n        h_dst = x[:mfgs[0].num_dst_nodes()]\n        h = self.conv1(mfgs[0], (x, h_dst))\n        h = F.relu(h)\n        h_dst = h[:mfgs[1].num_dst_nodes()]\n        h = self.conv2(mfgs[1], (h, h_dst))\n        return h\n\nmodel = Model(num_features, 128).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the Score Predictor for Edges\n--------------------------------------\n\nAfter getting the node representation necessary for the minibatch, the\nlast thing to do is to predict the score of the edges and non-existent\nedges in the sampled minibatch.\n\nThe following score predictor, copied from the :doc:`link prediction\ntutorial <../blitz/4_link_predict>`, takes a dot product between the\nincident nodes\u2019 representations.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import dgl.function as fn\n\nclass DotPredictor(nn.Module):\n    def forward(self, g, h):\n        with g.local_scope():\n            g.ndata['h'] = h\n            # Compute a new edge feature named 'score' by a dot-product between the\n            # source node feature 'h' and destination node feature 'h'.\n            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n            return g.edata['score'][:, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluating Performance (Optional)\n---------------------------------\n\nThere are various ways to evaluate the performance of link prediction.\nThis tutorial follows the practice of `GraphSAGE\npaper <https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf>`__,\nwhere it treats the node embeddings learned by link prediction via\ntraining and evaluating a linear classifier on top of the learned node\nembeddings.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To obtain the representations of all the nodes, this tutorial uses\nneighbor sampling as introduced in the :doc:`node classification\ntutorial <L1_large_node_classification>`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>If you would like to obtain node representations without\n   neighbor sampling during inference, please refer to this `user\n   guide <guide-minibatch-inference>`.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def inference(model, graph, node_features):\n    with torch.no_grad():\n        nodes = torch.arange(graph.number_of_nodes())\n\n        sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4])\n        train_dataloader = dgl.dataloading.NodeDataLoader(\n            graph, torch.arange(graph.number_of_nodes()), sampler,\n            batch_size=1024,\n            shuffle=False,\n            drop_last=False,\n            num_workers=4,\n            device=device)\n\n        result = []\n        for input_nodes, output_nodes, mfgs in train_dataloader:\n            # feature copy from CPU to GPU takes place here\n            inputs = mfgs[0].srcdata['feat']\n            result.append(model(mfgs, inputs))\n\n        return torch.cat(result)\n\nimport sklearn.metrics\n\ndef evaluate(emb, label, train_nids, valid_nids, test_nids):\n    classifier = nn.Linear(emb.shape[1], num_classes).to(device)\n    opt = torch.optim.LBFGS(classifier.parameters())\n\n    def compute_loss():\n        pred = classifier(emb[train_nids].to(device))\n        loss = F.cross_entropy(pred, label[train_nids].to(device))\n        return loss\n\n    def closure():\n        loss = compute_loss()\n        opt.zero_grad()\n        loss.backward()\n        return loss\n\n    prev_loss = float('inf')\n    for i in range(1000):\n        opt.step(closure)\n        with torch.no_grad():\n            loss = compute_loss().item()\n            if np.abs(loss - prev_loss) < 1e-4:\n                print('Converges at iteration', i)\n                break\n            else:\n                prev_loss = loss\n\n    with torch.no_grad():\n        pred = classifier(emb.to(device)).cpu()\n        label = label\n        valid_acc = sklearn.metrics.accuracy_score(label[valid_nids].numpy(), pred[valid_nids].numpy().argmax(1))\n        test_acc = sklearn.metrics.accuracy_score(label[test_nids].numpy(), pred[test_nids].numpy().argmax(1))\n    return valid_acc, test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining Training Loop\n----------------------\n\nThe following initializes the model and defines the optimizer.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = Model(node_features.shape[1], 128).to(device)\npredictor = DotPredictor().to(device)\nopt = torch.optim.Adam(list(model.parameters()) + list(predictor.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following is the training loop for link prediction and\nevaluation, and also saves the model that performs the best on the\nvalidation set:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tqdm\nimport sklearn.metrics\n\nbest_accuracy = 0\nbest_model_path = 'model.pt'\nfor epoch in range(1):\n    with tqdm.tqdm(train_dataloader) as tq:\n        for step, (input_nodes, pos_graph, neg_graph, mfgs) in enumerate(tq):\n            # feature copy from CPU to GPU takes place here\n            inputs = mfgs[0].srcdata['feat']\n\n            outputs = model(mfgs, inputs)\n            pos_score = predictor(pos_graph, outputs)\n            neg_score = predictor(neg_graph, outputs)\n\n            score = torch.cat([pos_score, neg_score])\n            label = torch.cat([torch.ones_like(pos_score), torch.zeros_like(neg_score)])\n            loss = F.binary_cross_entropy_with_logits(score, label)\n\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n\n            tq.set_postfix({'loss': '%.03f' % loss.item()}, refresh=False)\n\n            if (step + 1) % 500 == 0:\n                model.eval()\n                emb = inference(model, graph, node_features)\n                valid_acc, test_acc = evaluate(emb, node_labels, train_nids, valid_nids, test_nids)\n                print('Epoch {} Validation Accuracy {} Test Accuracy {}'.format(epoch, valid_acc, test_acc))\n                if best_accuracy < valid_acc:\n                    best_accuracy = valid_acc\n                    torch.save(model.state_dict(), best_model_path)\n                model.train()\n\n                # Note that this tutorial do not train the whole model to the end.\n                break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusion\n----------\n\nIn this tutorial, you have learned how to train a multi-layer GraphSAGE\nfor link prediction with neighbor sampling.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Thumbnail Courtesy: Link Prediction with Neo4j, Mark Needham\n# sphinx_gallery_thumbnail_path = '_static/blitz_4_link_predict.png'"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}