{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nIntroduction of Neighbor Sampling for GNN Training\n==================================================\n\nIn :doc:`previous tutorials <../blitz/1_introduction>` you have learned how to\ntrain GNNs by computing the representations of all nodes on a graph.\nHowever, sometimes your graph is too large to fit the computation of all\nnodes in a single GPU.\n\nBy the end of this tutorial, you will be able to\n\n-  Understand the pipeline of stochastic GNN training.\n-  Understand what is neighbor sampling and why it yields a bipartite\n   graph for each GNN layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Message Passing Review\n----------------------\n\nRecall that in `Gilmer et al. <https://arxiv.org/abs/1704.01212>`__\n(also in :doc:`message passing tutorial <../blitz/3_message_passing>`), the\nmessage passing formulation is as follows:\n\n\\begin{align}m_{u\\to v}^{(l)} = M^{(l)}\\left(h_v^{(l-1)}, h_u^{(l-1)}, e_{u\\to v}^{(l-1)}\\right)\\end{align}\n\n\\begin{align}m_{v}^{(l)} = \\sum_{u\\in\\mathcal{N}(v)}m_{u\\to v}^{(l)}\\end{align}\n\n\\begin{align}h_v^{(l)} = U^{(l)}\\left(h_v^{(l-1)}, m_v^{(l)}\\right)\\end{align}\n\nwhere DGL calls $M^{(l)}$ the *message function*, $\\sum$ the\n*reduce function* and $U^{(l)}$ the *update function*. Note that\n$\\sum$ here can represent any function and is not necessarily a\nsummation.\n\nEssentially, the $l$-th layer representation of a single node\ndepends on the $(l-1)$-th layer representation of the same node,\nas well as the $(l-1)$-th layer representation of the neighboring\nnodes. Those $(l-1)$-th layer representations then depend on the\n$(l-2)$-th layer representation of those nodes, as well as their\nneighbors.\n\nThe following animation shows how a 2-layer GNN is supposed to compute\nthe output of node 5:\n\n|image1|\n\nYou can see that to compute node 5 from the second layer, you will need\nits direct neighbors\u2019 first layer representations (colored in yellow),\nwhich in turn needs their direct neighbors\u2019 (i.e.\u00a0node 5\u2019s second-hop\nneighbors\u2019) representations (colored in green).\n\n.. |image1| image:: https://data.dgl.ai/tutorial/img/sampling.gif\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neighbor Sampling Overview\n--------------------------\n\nYou can also see from the previous example that computing representation\nfor a small number of nodes often requires input features of a\nsignificantly larger number of nodes. Taking all neighbors for message\naggregation is often too costly since the nodes needed for input\nfeatures would easily cover a large portion of the graph, especially for\nreal-world graphs which are often\n`scale-free <https://en.wikipedia.org/wiki/Scale-free_network>`__.\n\nNeighbor sampling addresses this issue by selecting a subset of the\nneighbors to perform aggregation. For instance, to compute\n$\\boldsymbol{h}_8^{(2)}$, you can choose two of the neighbors\ninstead of all of them to aggregate, as in the following animation:\n\n|image2|\n\nYou can see that this method uses much fewer nodes needed in message\npassing for a single minibatch.\n\n.. |image2| image:: https://data.dgl.ai/tutorial/img/bipartite.gif\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also notice in the animation above that the computation\ndependencies in the animation above can be described as a series of\nbipartite graphs.\nThe output nodes (called *destination nodes*) are on one side and all the\nnodes necessary for inputs (called *source nodes*) are on the other side.\nThe arrows indicate how the sampled neighbors propagates messages to the nodes.\nDGL calls such graphs *message flow graphs* (MFG).\n\nNote that some GNN modules, such as `SAGEConv`, need to use the destination\nnodes' features on the previous layer to compute the outputs.  Without\nloss of generality, DGL always includes the destination nodes themselves\nin the source nodes.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What\u2019s next?\n------------\n\n:doc:`Stochastic GNN Training for Node Classification in\nDGL <L1_large_node_classification>`\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Thumbnail Courtesy: Understanding graph embedding methods and their applications, Mengjia Xu\n# sphinx_gallery_thumbnail_path = '_static/large_L0_neighbor_sampling_overview.png'"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}